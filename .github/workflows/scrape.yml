name: Job Scraper

on:
  schedule:
    # Run every 6 hours
    - cron: "0 */6 * * *"
  workflow_dispatch: # allow manual trigger from GitHub UI

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      # ── Checkout ────────────────────────────────────────────────────────────
      - name: Checkout repository
        uses: actions/checkout@v4

      # ── Python setup ────────────────────────────────────────────────────────
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip
          cache-dependency-path: job_scraper/requirements.txt

      # ── Install Python dependencies ─────────────────────────────────────────
      - name: Install dependencies
        working-directory: job_scraper
        run: pip install -r requirements.txt

      # ── Install Playwright browsers ─────────────────────────────────────────
      - name: Install Playwright Chromium
        working-directory: job_scraper
        run: |
          # Ubuntu 24.04 renamed libasound2 to libasound2t64; install it first
          sudo apt-get update -qq
          sudo apt-get install -y libasound2t64 || true
          playwright install chromium --with-deps

      # ── Write secrets to .env ───────────────────────────────────────────────
      - name: Write .env file
        working-directory: job_scraper
        env:
          GMAIL_ADDRESS: ${{ secrets.GMAIL_ADDRESS }}
          GMAIL_APP_PASSWORD: ${{ secrets.GMAIL_APP_PASSWORD }}
          ADZUNA_APP_ID: ${{ secrets.ADZUNA_APP_ID }}
          ADZUNA_APP_KEY: ${{ secrets.ADZUNA_APP_KEY }}
          JOOBLE_API_KEY: ${{ secrets.JOOBLE_API_KEY }}
        run: |
          cat > .env <<EOF
          GMAIL_ADDRESS=$GMAIL_ADDRESS
          GMAIL_APP_PASSWORD=$GMAIL_APP_PASSWORD
          ADZUNA_APP_ID=$ADZUNA_APP_ID
          ADZUNA_APP_KEY=$ADZUNA_APP_KEY
          JOOBLE_API_KEY=$JOOBLE_API_KEY
          EOF

      # ── Write Google service account JSON (if secret is set) ────────────────
      - name: Write Google service account JSON
        working-directory: job_scraper
        env:
          GOOGLE_SERVICE_ACCOUNT_JSON_CONTENT: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}
        run: |
          if [ -n "$GOOGLE_SERVICE_ACCOUNT_JSON_CONTENT" ]; then
            echo "$GOOGLE_SERVICE_ACCOUNT_JSON_CONTENT" > service_account.json
            echo "service_account.json written"
          else
            echo "GOOGLE_SERVICE_ACCOUNT_JSON secret not set"
          fi

      # ── Run the scraper ─────────────────────────────────────────────────────
      - name: Run job scraper
        working-directory: job_scraper
        run: python main.py

      # ── Upload SQLite DB as artifact ─────────────────────────────────────────
      - name: Upload database artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: jobs-db-${{ github.run_number }}
          path: job_scraper/jobs.db
          retention-days: 7

      # ── Upload log as artifact ───────────────────────────────────────────────
      - name: Upload scraper log
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scraper-log-${{ github.run_number }}
          path: job_scraper/scraper.log
          retention-days: 3
